{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b70901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roku/miniconda3/envs/dlproj/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4265b9",
   "metadata": {},
   "source": [
    "# Creating new datasets\n",
    "\n",
    "The goal of this notebook is to make two unified datasets out of the following larger datasets: MMLU, TruthfulQA, HellaSwag. \n",
    "\n",
    "The first CSV should have the following columns: Type, Question, Options, Answer. Question refers to the actual prompt. Type refers to the category of question (math, history, law, fact, hellaswag). Options will be the multiple choice options. Answer will be the correct answer, in 1/2/3/4 format. This dataset will be evaluated using probability scoring each option.\n",
    "\n",
    "The second CSV will be focused on generation quality. This will also be made from TruthfulQA, HellaSwag, etc, but will utilize an LLM-as-a-judge for generation quality estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a00da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a9040",
   "metadata": {},
   "source": [
    "## 1. MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dc07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clinical = load_dataset(\"cais/mmlu\", \"clinical_knowledge\")['test'].to_pandas()\n",
    "data_law = load_dataset(\"cais/mmlu\", \"international_law\")['test'].to_pandas()\n",
    "data_cs = load_dataset(\"cais/mmlu\", \"college_computer_science\")['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f99080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>subject</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What size of cannula would you use in a patien...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>[18 gauge., 20 gauge., 22 gauge., 24 gauge.]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The key attribute in successful marathon runni...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>[strength., power., stride length., stamina.]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following is the commonest cause ...</td>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>[Alzheimer's disease., Cerebrovascular (stroke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question             subject  \\\n",
       "0  What size of cannula would you use in a patien...  clinical_knowledge   \n",
       "1  The key attribute in successful marathon runni...  clinical_knowledge   \n",
       "2  Which of the following is the commonest cause ...  clinical_knowledge   \n",
       "\n",
       "                                             choices  answer  \n",
       "0       [18 gauge., 20 gauge., 22 gauge., 24 gauge.]       0  \n",
       "1      [strength., power., stride length., stamina.]       3  \n",
       "2  [Alzheimer's disease., Cerebrovascular (stroke...       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clinical.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48b8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add each of the dataframes to the final dataframe\n",
    "df_final = pd.concat([df_final, data_clinical, data_law, data_cs])\n",
    "\n",
    "# rename the answer column to answer\n",
    "df_final = df_final.rename(columns={'question': 'Question', 'subject': 'Type', 'choices': 'Options', 'answer': 'Answer'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da844638",
   "metadata": {},
   "source": [
    "# 2. TruthfulQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc72a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_truth = load_dataset(\"EleutherAI/truthful_qa_mc\")['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3019b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_truth = data_truth.rename(columns={'question': 'Question', 'choices': 'Options', 'label': 'Answer'}, inplace=False)\n",
    "data_truth['Type'] = 'TruthfulQA'\n",
    "\n",
    "df_final = pd.concat([df_final, data_truth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9fff5",
   "metadata": {},
   "source": [
    "## 3. HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913bf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of data from hellaswag (only 400 rows)\n",
    "data_hs = load_dataset(\"Rowan/hellaswag\", split=\"train\").to_pandas()[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba0e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hs = data_hs.rename(columns={'ctx': 'Question', 'endings': 'Options', 'label': 'Answer'}, inplace=False)\n",
    "data_hs['Type'] = 'HellaSwag'\n",
    "data_hs = data_hs[['Question', 'Options', 'Answer', 'Type']]\n",
    "\n",
    "df_final = pd.concat([df_final, data_hs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b2896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_json(\"data_mc.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f97ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
